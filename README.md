# Crowdfunding_ETL
Project Two: Sophia Emanuel and Amanda Liu.

For the first part we worked on creating the Category and Subcategory dataframes. To do this we first found the columns by using .columns on our dataframe. Then we split the 'category & sub-category' by using .str.split and splitting on the / to create seperate columns for categories and subcategories. We then used .unique on both the categories and subcategories to find each of the unique values and then found the amount of disticnt values that we have. We then used np.arange from 1-10 for categories and 1-25 for subcategories inorder to create an numpy array. With this array, we created a list comprehension to add cat to each category id and subcat to each subcategory and then put each of these into their own dataframe, being named category_df and subcategory_df. With this, we exported these as CSV files into the resources file as files named 'category.csv' and 'subcategory.csv'

For the second part we created a Campaign dataframe. To do this we first copied the crowdfunding_info_df dataframe and then renamed the column blurb to description, launched_at to launched_date, and deadline to end_date. Then, we used .dtypes to check the tyes in our campaign types and used .astype(float) to change goal and pledged from int64 to float types. Then we changed launched_date and end_date to datetime format by using pd.to_datetime and .dt.strftime('%Y-%m-%d') to change to a year-month-day format. With our converted data we then merged our campaign and category dataframes on category and then merged our first merge with our subcategory dataframe on subcategory. We then dropped the unwanted columns by using .drop and axis=1 to drop the labels. Finally, we exported the cleaned dataframe into a csvfile in the resources file under the name 'campaign.csv.'

For the third part, we chose option 2 using regular expression to create a Contacts DataFrame. We first extracted the contact_id by creating a new column and using str.extract and specifying that we wanted 4 digits and changing the dtype to int, so that it can be searched later on in its csvfile. Then we extracted the names by the same method only to use {r'"name": \s*"(.*?)"'} as we looked for all the values associated with the key "name". Once that is done, we repeat the same process to extract the emails. Then we ordered the columns to contact_id, name, then email. Then we used str.split(' ') to split the name at the space into a "first_name" column and a "last_name" column, and dropped the original "name" column. Lastly, we reordered the columns so that the emails appear last, and then we exported the Contacts Dataframe into a csvfile named "contacts.csv"

We completed the next part in QuickDBD and postgres PgAdmin. We first made an ERD after inspecting all the csvfiles we had exported earlier. Through that we were able to download and import the tables into PgAdmin. We modified the tables and verified the primary keys and foreign keys in order. After that, we imported all the csv files that we had created into each table. The first try for the campaign table wasn't successful as the "lauched_date" and "end_date" column format had turned into Excel's default format for dates (M/DD/YYYY). After adjusting it directly in the csv file in Excel to YYYY-MM-DD, 'campaign.csv' had imported successfully. We ran "SELECT *" for each table and it returned our expected data. 
